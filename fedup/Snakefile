import os
import json
import time
import signal
import pandas
import requests
import threading
import subprocess

TIMEOUT = config.setdefault("timeout", 600) # 10 minutes
XPDIR = config.setdefault("xpdir", "output")
RESTART = config.setdefault("restart", False)

def get_pid(port):
    pid = None
    try:
        output = subprocess.check_output([
            "lsof", "-t", "-i", f":{port}",
            "-sTCP:LISTEN"])
        pid = int(output.strip())
    except subprocess.CalledProcessError:
        pass
    return pid

def start_virtuoso():
    if get_pid(8890) is not None:
        print("Virtuoso is running")
        return
    print("Running virtuoso")
    with open(".log/virtuoso.log", "a") as logfile:
        subprocess.Popen([
            "/GDD/virtuoso/bin/virtuoso-t",
            "+configfile", "/GDD/virtuoso/var/lib/virtuoso/db/fedup.ini",
            "+foreground"],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL)
            # stdout=logfile.fileno(),
            # stderr=logfile.fileno())
    time.sleep(20)

def start_spring():
    if get_pid(8080) is not None:
        print("Spring is running")
        return
    print("Running spring")
    with open(".log/fedup.log", "a") as logfile:
        subprocess.Popen([
            "mvn", "spring-boot:run",
            "-Dspring-boot.run.jvmArguments=\"-Xms4096M\"",
            "-Dspring-boot.run.jvmArguments=\"-Xmx8192M\"",
            "-Dspring-boot.run.jvmArguments=\"-XX:TieredStopAtLevel=4\""],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL)
            # stdout=logfile.fileno(),
            # stderr=logfile.fileno())
    time.sleep(10)

def kill_virtuoso_and_spring():
    pid = get_pid(8080)
    if pid:
        os.kill(pid, signal.SIGKILL)
    pid = get_pid(8890)
    if pid:
        os.kill(pid, signal.SIGKILL)

def delayed_task(delay, cancel_event):
    if not cancel_event.wait(delay):
        print("timeout reached, killing virtuoso and spring")
        kill_virtuoso_and_spring()

def list_queries(workload):
    queries = []
    for query in os.listdir(f"queries/{workload}"):
        queries.append(query.split('.')[0])
    return queries

def todo(wcs):
    files = []
    for workload in config.setdefault("workload", ["fedbench"]):
        for batch in config.setdefault("batch", ["batch0"]):
            for approach in config.setdefault("approach", ["fedup-optimal"]):
                for query in list_queries(workload):
                    for run in config.setdefault("run", [1]):
                        files.append(f"{XPDIR}/{workload}/{batch}/{approach}/{query}.{run}.csv")
    return files

def load_blacklist(wcs):
    if not os.path.exists(f"{wcs.xpdir}/.blacklist"):
        bl = []
    else:
        with open(f"{wcs.xpdir}/.blacklist", "r") as reader:
            bl = json.load(reader)
    return bl

def save_blacklist(wcs, bl):
    with open(f"{wcs.xpdir}/.blacklist", "w") as writer:
        json.dump(bl, writer, indent=2)

def is_blacklist(wcs):
    bl = load_blacklist(wcs)
    eq_keys = wcs.keys() - ["batch", "reason", "run"]
    for item in bl:
        if all(wcs[k] == item[k] for k in eq_keys) and wcs["batch"] >= item["batch"]:
            return item["reason"]
    return None

def blacklist(wcs, reason):
    bl = load_blacklist(wcs)
    if not is_blacklist(wcs):
        data = dict(wcs.items())
        data["reason"] = reason
        bl.append(data)
        save_blacklist(wcs, bl)
    print(f"blacklisted for {reason}")

def zero_result(status):
    return {
        "status": status,
        "sourceSelectionTime": 0,
        "executionTime": 0,
        "numASKQueries": 0,
        "numSolutions": 0,
        "numAssignments": 0,
        "tpwss": 0,
        "provenanceMappings": "[]",
        "solutions": "[]",
        "assignments": "[]"}

def extend_and_format_result(result, wcs):
    columns = list(result.keys())
    row = list(result.values())
    columns.extend(["query", "approach", "run", "workload", "batch"])
    row.extend([wcs.query, wcs.approach, wcs.run, wcs.workload, wcs.batch])
    return pandas.DataFrame([row], columns=columns)

def print_dataframe(df):
    for column in df.columns:
        print(f"df[{column}] = {df[column].to_string(index=False)}")

def run(params):
    # starting servers in case they have been killed by the timeout policy
    start_virtuoso()
    start_spring()
    # setting up the timeout policy
    cancel_event = threading.Event()
    delayed_task_thread = threading.Thread(
        target=delayed_task,
        args=(TIMEOUT, cancel_event))
    delayed_task_thread.start()
    # running the query
    start_time = time.time()
    try:
        response = requests.post("http://localhost:8080/fedSparql", json=params)
        # response = requests.get("http://localhost:8080/fedSparql", params=params)
        result = json.loads(response.content.decode("utf-8"))
    except Exception as error:
        elapsed_time = time.time() - start_time
        result = zero_result("TIMEOUT" if elapsed_time >= TIMEOUT else "ERROR")   
    # cancelling the timeout policy if everything went well 
    cancel_event.set()
    delayed_task_thread.join()
    # to deal with CostFed issues...
    if RESTART:
        kill_virtuoso_and_spring()
    return result
    
wildcard_constraints:
    query = "[A-z0-9]+",
    run = "[0-9]+",
    workload = "(fedbench|fedshop)",
    batch = "batch[0-9]",
    approach = "(fedx|hibiscus(-index)?|costfed(-index|-noopt)?|fedup(-h0)?-optimal)",
    xpdir = "[A-z0-9\-]+"

rule all:
    input: todo

rule merge_all:
    input: todo
    output: "{xpdir}/data.csv"
    run:
        dataframes = []
        projected_columns = [
            "query", "approach", "run", "workload", "batch",
            "sourceSelectionTime", "executionTime", "numASKQueries", "tpwss"]
        for file in input:
            try:
                df = pandas.read_csv(file)
                dataframes.append(df[projected_columns])
            except Exception as error:
                print(f"error in file {file}: {error}")
                raise error
        pandas.concat(dataframes).to_csv(str(output))

rule run_query:
    input:
        query = "queries/{workload}/{query}.sparql",
        config = "config/{workload}/{batch}/{approach}.props",
        endpoints = "config/{workload}/{batch}/endpoints.txt"
    output: "{xpdir}/{workload}/{batch}/{approach}/{query}.{run}.csv"
    run:
        reason = is_blacklist(wildcards)
        if reason is None:
            with open(str(input.query)) as reader:
                query = reader.read()
            result = run({
                "queryString": query,
                "configFileName": input.config,
                "endpointsFileName": input.endpoints,
                "assignments": [],
                "runQuery": True})
            if result["status"] != "OK":
                blacklist(wildcards, result["status"])
        else:
            print(f"skip because blacklisted for {reason}")
            result = zero_result(reason)
        df = extend_and_format_result(result, wildcards)
        print_dataframe(df)
        df.to_csv(str(output))

rule fedup_random_walks_efficiency:
    input:
        query = "queries/{workload}/{query}.sparql",
        config = "config/{workload}/{batch}/{approach}.props",
        endpoints = "config/{workload}/{batch}/endpoints.txt",
        optimal = "{xpdir}/{workload}/{batch}/fedup-optimal/{query}.1.csv"
    output: "{xpdir}/{workload}/{batch}/{approach,fedup(-h0)?}/{query}.{run}.csv"
    run:
        reason = is_blacklist(wildcards)
        if reason is None:
            with open(str(input.query)) as reader:
                query = reader.read()
            optimalAssignments = pandas.read_csv(str(input.optimal))["assignments"].values[0]
            optimalAssignments = optimalAssignments.replace("\'", "\"")
            optimalAssignments = json.loads(optimalAssignments)
            result = run({
                "queryString": query,
                "configFileName": input.config,
                "endpointsFileName": input.endpoints,
                "assignments": optimalAssignments,
                "runQuery": True})
            if result["status"] != "OK":
                blacklist(wildcards, result["status"])
        else:
            print(f"skip because blacklisted for {reason}")
            result = zero_result(reason)
        df = extend_and_format_result(result, wildcards)
        print_dataframe(df)
        df.to_csv(str(output))